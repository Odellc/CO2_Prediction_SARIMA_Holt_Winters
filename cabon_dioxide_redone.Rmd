---
title: 'Final Project: Motor Gasoline'
author: "Odell, Christopher"
date: "March 10, 2019"
output:
  word_document: default
  pdf_document: default
  html_document: default
---

```{R include=FALSE}
knitr::opts_chunk$set(echo=FALSE, message = FALSE,
warning = FALSE,  fig.height = 4, fig.width = 7) 

```

```{r, include=FALSE}
library(TSA)
library(forecast)
library(tidyr)
library(dplyr)

#Read in Data
df <- read.csv("environment-carbon-dioxide-emiss.csv", head = T, stringsAsFactors = FALSE)

# Rename a column in R
colnames(df)[colnames(df)=="Environment..carbon.dioxide.emissions.from.energy.consumption.by.source"] <- "CO2"

df_ts <-  ts(df$CO2[-487], start = c(1973, 1), frequency = 12) # remove NA at observation 487
```


#Introduction

In this paper we conduct a time series analysis of monthly motor vehicle CO2 emissions in metric Megatons from gasoline (excluding ethanol) for the time period between January 1973 to June 2013. We use four different time series analysis tools to gain a deeper understanding of this data, and forecast 24 months of future carbon dioxide emission levels.  The methods we examine in this analysis are ARMA modelling, (S)ARIMA modelling, Exponential Smoothing and Spectral Analysis. Each method provides a different approach to examining the time series, and our aim is to demonstrate how these various models contribute to the analysis and forecasting of the time series.  In end we will provide a recommendation on which one(s) to use for forecasting. 

Below is a time plot for the CO2 emissions data. The trend appears to follow a polynomial pattern over time and there is a clear seasonal component. Variance appears to be consistent and there does not appear to be a need for a log transformation, so we will proceed with the data in its natural scale. 


#Data Description

The data for this analysis comes from the United States Energy Information Administration. The focus is on carbon dioxide emissions ($CO_2$) from energy consumption by source with an emphasis on motor gasoline, excluding ethanol. The $CO_2$ emissions are in the unit measurement of million megatonne (Mt). I really wanted to focus on environmental data and what the prediction of this impact might look like overtime. I wanted to really focus on a data set that was going to incorporate multiple aspects of time series analysis and so this data fits the analysis for a couple of reasons. One there appeared to be a change over time in the mean of the $CO_2$ units. Second, when looking at the original data as shown below (Figure 1), I saw a visual seasonality. Another detail that I had noticed while examining the series was that there appeared to be a polynomial pattern over time. Performing time series analysis, does require that some assumptions are met. There needed to be enough observations, preferably over 300, and the data gathered has 486 observations which met the length requirement. Another assumption is that the covariance and variance do not change overtime. To meet this assumption there will need to be various time series techniques applied to the data to ensure stationarity. (e.g. differencing, decomposing). 



```{r, echo=FALSE, fig.height=3}
#
# INTRODUCTION
#
#Plot the time series
plot(df_ts, ylab = "CO2, metric Megatons", main= "Motor Vehicle CO2 from 1973 - 2013 " )
log_df_ts <- log(df_ts)
title(sub = "Figure 1",
      cex.sub = 0.75, font.sub = 3, col.sub = "black")
mtext("Source: US Energy Info. Admin.", side=1, adj=1.21, line=4, cex=0.65, font=1);

```

#Model Description:

Since the goal of the analysis was to predict 24 months of future carbon dioxide emission levels using different time series analysis tools, I need to evaluate each of these tools separately. The different tools that are going to be used are: ARMA, (S)ARIMA, Exponential Smoothing, and Spectral Analysis. 
The first tool is using the ARMA(p,q) modeling, which is comprised of the Autoregressive model (AR) and the Moving-average model (MA). The p term of the ARMA is the portion of the AR while the q term is for the MA portion. 

Since the goal of the analysis was to predict 24 months of future carbon dioxide emission levels using different time series analysis tools, I need to evaluate each of these tools separately. 

The first tool is using the ARMA(p,q) modeling, which is comprised of the Autoregressive model (AR) and the Moving-average model (MA). The p term of the ARMA is the portion of the AR while the q term is for the MA portion.

$X_t =  \mu + \epsilon_t + \sum\limits_{i=1}^{q}\theta_i\epsilon_{t-i}$

This can be broken down into two parts:

AR model:

$\phi(B) = 1 - \alpha_1B - \alpha_2B^2 - ... - \alpha_pB^p$

MA model:

$\theta(B) = 1 - \beta_1B - \beta_2B^2 - ... - \beta_qB^q$

Where the q, p are the terms of the ARMA model, the $\phi$ is the Autoregressive operator, and $\theta$ is the Moving-average operator. Addtionally, $\alpha$ is the coefficients for the Autoregression portion of the model and $\beta$ is the coefficietns for the Moving-average porption of the model. 


The second tool is the $(S)ARIMA(p,d,q) \times (P,D,Q)_s$ model. With AR and MA being the same components of the ARMA model. There some additonal pieces added where the S is added if there is a seasonality compnent, which does apply in this analysis, and the I denoted as (d and D) is the differencing tool. The small (p,d,q) is represented as teh short term correlations (non-seasonality) where the capital (P,D,Q) is the correlations over multiple seasons (seasonality). The s is used to control what type of seasonality that I am dealing with and in this case is a yearly so s is noted as being 12.

The model is given by:

$\Phi(B^2)\phi(B)\triangledown\overset{D}{s}\triangledown^dX_t = \Theta(B^s)\theta(B)W_t$

This is made up of the non-seasonal operators: $\phi(B)$ and $\theta(B)$ with the seasonal operators $\Phi(B^s)$ and $\Theta(B^s)$, the portion of the model that is $\triangledown\overset{D}{s}\triangledown^dX_t$ is an seasonal $ARMA(p,q)\times(P,Q)_s$

The third method mentioned is the Holt-Winters - Exponential smoothing. The model is broken down into three parts. Each model part is a smoothing method, one for the level $\ell_{t}$, one for the trend $b_{t}$ , and one for the seasonal component $s_{t}$ , with corresponding smoothing parameters $\alpha$, $\beta^*$ and $\gamma$. I use m to denote the frequency of the seasonality, i.e., the number of seasons in a year.


$$\begin{align*}
  \hat{y}_{t+h|t} &= \ell_{t} + hb_{t} + s_{t+h-m(k+1)} \\
  \ell_{t} &= \alpha(y_{t} - s_{t-m}) + (1 - \alpha)(\ell_{t-1} + b_{t-1})\\
  b_{t} &= \beta^*(\ell_{t} - \ell_{t-1}) + (1 - \beta^*)b_{t-1}\\
  s_{t} &= \gamma (y_{t}-\ell_{t-1}-b_{t-1}) + (1-\gamma)s_{t-m},
\end{align*}$$

where k is the integer part of $(h-1)/m$, this ensures that the estimates of the seasonal indices used for forecasting come from the final year of the sample. The level equation shows a weighted average between the seasonally adjusted observation $(y_{t} - s_{t-m})$ and the non-seasonal forecast $(\ell_{t-1}+b_{t-1})$ for time t .The seasonal equation shows a weighted average between the season, $(y_{t}-\ell_{t-1}-b_{t-1})$, and the seasonal before it last year.

The forth method is the Spectral Analysis. 

$$\begin{align*}

\gamma(k) = \int_\pi^0 \!cos(wk)dF(w)\\

f(w) = \frac{dF(w)}{dw}\\


\end{align*}$$

$\gamma(k)$ is the auto-covaraince function, $F(w)$ is the spectral distribution function and is the contribution to the variance from the frequency of 0 to $w$. And $f(w)dw$ is approximtely the contribution on the variance with frequencies in the range $(w, w+dw)$

This four different model methods were used to gather a deeper understanding of the data, and to find a model for prediction. 


###ARMA Modelling

####Decomposing Trend

As noted above, I observed both trend and seasonality components in the emissions data. To fit an ARMA model, I needed to first estimate the trend using a locally weighted regression (loess) on a monthly frequency. The loess, a type of smoothing method, was selected after also considering a linear model due to two advantages: first, the non-parametric features of the loess smooth are appropriate for both normal and non-normally distributed data, and second, a linear model would not be appropriate for this dataset due to large fluctuations in the long term mean change in the emissions. Next, we plotted the loess against the observed values as shown below. When selected the level of smoothing for the loess it is important to address this process from multiple angles. The controlling mechanisim in the loess is the span which gave me the ability to control a level of fit to remove the trend from the data. The loess model fits would be subtracted from the time series model to remove trend, the challenge is finding a good enough of a fit for the loess. 

I addressed this issue with comparing eight different model fits, with the caution that I could overfit the model in mind. I know that the lower the span of the model the rougher the model would get and the closer I would get to overfitting. I started in the range of 0.30 and worked down in increments of 2.5. I found that 0.30 was not sufficent enough to remove the trend, as it left the auto-correlation function (ACF) plot with a pattern far outside of what I had expected. When I got as low as 0.15 I really noticed the overfitting of the data with the pattern drastically changing the ACF results for the worse. The most appropriate span I found for the loess model was 0.20 which gave me a model fit that seemed to control the results of the ACF and partial autocorrelation function (PACF) with the most accurancy which is the level that I had proceeded forward with. 


The plot series below (Figure 2) shows a smooth trend, with the blue line showing the estimated trend and the black points and line showing the emissions time series. The loess on a monthly frequency was a close fit to the actual observations. Therefore, I removed the estimated trend from the observed emissions data and proceeded with the analysis. The second plot (Figure 3) below shows the time series excluding the estimated trend.  


```{r, echo=FALSE}
#
# ARMA
#

#Estimate trend using a LOESS fit
df_ts.time <- time(df_ts) # Get the time span for the loess regression
df_ts.loess <- loess(df_ts ~ df_ts.time, span = 0.20)
df_ts.loess1 <- loess(df_ts ~ df_ts.time, span = 0.30)
df_ts.loess2 <- loess(df_ts ~ df_ts.time, span = 0.20)
df_ts.loess3 <- loess(df_ts ~ df_ts.time, span = 0.25)
df_ts.loess.pred <- predict(df_ts.loess,newdata=df_ts.time) # Predict the trend
df_ts.loess.trend <- ts(df_ts.loess.pred, start = c(1973, 1),  frequency = 12)# Monthly Freq

par(mfrow = c(1, 2)) 

# overlay the trend on the time plot
plot(df_ts, xlab = "Year", ylab = "CO2, metric Megatons",
     main = "CO2 Emissions, Jan-73 to Jun-13",type='o') # Simple Time-Series
lines(df_ts.loess.trend, col = "blue", lty = 1, lwd = 4)
title(sub = "Figure 2",
      cex.sub = 0.75, font.sub = 3, col.sub = "black")

trend.res <- df_ts.loess$residuals
# change it into time series
trend.res <- ts(trend.res, start = c(1973, 1), deltat = 1/12)
# plot it
plot(trend.res, xlab = "Year", ylab = "Residuals after removing trend", main="LOESS Residuals")
title(sub = "Figure 3",
      cex.sub = 0.75, font.sub = 3, col.sub = "black")

anova(df_ts.loess,df_ts.loess1,df_ts.loess2, df_ts.loess3)


```


####Decomposing Seasonality

As seen in the residuals plot (Figure 3) of the data after removing the trend, there is still strong annual seasonality. This is not surprising, as driving habits typically fluctuate with the seasons. Therefore, to understand the seasonality of the $CO_2$ emissions, I calculated the 12-month moving average of the emissions of the time series without the trend and ploted five years (Figure 4) to demonstrate the shape of the seasonality. The resulting fit shows the decrease in driving during the off season appears to have a greater impact on the emissions than the increase in driving during the peak season. The impact is shown by how far the top peak is from zero and how far the bottom peak is from zero. Just as a reminder that the data started in June and not in Janaury.

```{r, echo=FALSE, fig.height=3}

mon <- cycle(df_ts)
# calculate the means at each month
res.lm <- lm(trend.res ~ factor(mon))
# deduct each point by the corresponding month mean
co2.season <- ts(res.lm$fitted.values, start = c(1973, 1) , deltat = 1/12)
# plot the seasonality, 5 years to get a feel for the shape
plot(co2.season, xlab = "Year", ylab = "Seasonality", main = "CO2 Seasonality after Estimated Trend Residuals", xlim = c(2000, 2005), ylim = c(-10, 5))
title(sub = "Figure 4",
      cex.sub = 0.75, font.sub = 3, col.sub = "black")

```



####Residual Analysis (Stationary Series)

The final step in fitting the ARMA model was to derive the stationary series (residual noise) by subtracting the seasonal fit from the emissions data that was already excluding the trend. To confirm the effectiveness of the ARMA model, I examined the mean, ACF, and PACF.  

Looking at the mean, it appears that the ARMA model effectively removed non-stationarity, because the mean is pratically zero at 1.480402e-17. The stationary series has a low point in 2009 which could be an outlier, with a couple of high points early on. However, looking at the ACF (Figure 6), where a unit lag is equal to one month, there is a strong correlation with previous months, indicating that the result after fitting the ARMA model is not stationary. The ACF is outside of the standard deviation after the initial decrease to zero, which occurs at a lag of 4. The ACF for the previous three months is the strongest, with a correlation value above 0.25. Further, there is a negative correlation beyond a lag of 9. The PACF (Figure 7) also shows a strong regular correlation with values above zero at regular intervals. This also indicates a non-stationary series, due to the fact that there is still regular correlation after removing the trend and seasonality. Therefore, I found the ARMA model to not be sufficient fit to this time series, and proceed forward to examine the fit of a Seasonal ARIMA model without conducting prediction. 

```{r, echo=FALSE, fig.height=3}

# remove the seasonality
co2.random <- ts(residuals(res.lm), start = c(1973, 1), deltat = 1/12)
mean(co2.random)
# plot the final series
plot(co2.random, xlab = "Year", ylab = "Residuals after removing seasonality", main = "Stationary Series, CO2 Emmissions from Jan 1973 to Jun 2013")
title(sub = "Figure 5",
      cex.sub = 0.75, font.sub = 3, col.sub = "black")

# correlation of residuals
par(mfrow = c(1, 2)) 
co2_acf <- acf(co2.random, na.action = na.pass, main = "Residual Noise, ACF")
title(sub = "Figure 6",
      cex.sub = 0.75, font.sub = 3, col.sub = "black")
co2_pacf <- pacf(co2.random, na.action = na.pass, "Residual Noise, PACF")
title(sub = "Figure 7",
      cex.sub = 0.75, font.sub = 3, col.sub = "black")

```


###Seasonal ARIMA Modeling

In the seasonal ARIMA model, it is not standard to remove the trend and seasonality like the ARMA approach discussed in the preceding section; instead I rely on differencing completed by the $(d,D)$ part of the model to produce a stationary series that fitted with the appropriate time series model. 

####Differencing to Remove Trend and Seasonality

To remove trend and seasonality I had considered differencing with lag 1 to remove the trend and then with lag 12 to remove the monthly seasonality. 

```{r , echo=FALSE}
#
#Seasonal ARIMA
#

#Reload data for SARIMA section of paper to avoid polluting other sections by re-using parameter names.
emission <-ts(df_ts, start=c(1973,1), frequency=12)

par(mfrow = c(1, 2)) 

#First order differencing
diff1 <- diff( emission, lag=1, differences=1)
plot(diff1,
     ylab="CO2, metric Megatons",
     xlab="Year",
     main="1st Order Differenced Data",
     type='l')
title(sub = "Figure 8",
      cex.sub = 0.75, font.sub = 3, col.sub = "black")

#Difference with lag 12 to address seasonality
diff12 <- diff(diff1, lag=12)
plot(diff12,
     xlab="Year",
     ylab="CO2, metric Megatons",
     main="1st and 12th Order Differencing",
     type='l')
title(sub = "Figure 9",
      cex.sub = 0.75, font.sub = 3, col.sub = "black")

```

The plot of the 1st order difference (Figure 8) is centered about zero and I did not see a visible change in the mean that would indicate that the trend was not been removed. The same can be said for the 1st and 12th order differencing plot (Figure 9), although I was suspicious still with the area around 2009 where variance decreases. But in general, the differencing suggested in the plots above appeared appropriate for the time series of this data.  

####Selecting a Time Series Model

To esnure the appropriateness of the model, I start by examining the ACF and PACF of the differenced data.

These plots are signifantly more 'messy' and more open to interpretation. Starting with the non-seasonal ARIMA model portion, the ACF (Figure 10) shows a cutoff after lag 1, which is indicative of a MA(1) model. The PACF (Figure 11) is a bit more complicated and I saw either a cutoff after lag 2 (suggesting AR(2)) or a decay that is indicative of an ARMA model.  

Looking at the seasonal portion of the model (Figure 10), the ACF seems to show a gradual decay which is indicative of an ARMA model.  The PACF (Figure 12) shows what might be an AR(2) behavior or perhaps nothing at all.  

```{r, echo=FALSE}
#ACF and PACF of differenced data

par(mfrow = c(1, 2)) 

acf(diff12,
    main="ACF For Differenced Series")
title(sub = "Figure 10",
      cex.sub = 0.75, font.sub = 3, col.sub = "black")
pacf(diff12,
     main="PACF For Differened Series")
title(sub = "Figure 11",
      cex.sub = 0.75, font.sub = 3, col.sub = "black")
```



Given this evidence, I examined the suitability of the following models:  

* Model 1: $ARIMA(1,1,1)x(1,1,1)_{12}$ 
* Model 2: $ARIMA(1,1,2)x(1,1,1)_{12}$ 
* Model 3: $ARIMA(0,1,1)x(1,1,1)_{12}$ 
* Model 4: $ARIMA(2,1,0)x(1,1,1)_{12}$ 
* Model 5: $ARIMA(1,1,1)x(2,1,1)_{12}$ 
* Model 6: $ARIMA(2,1,2)x(2,1,1)_{12}$ 
* Model 7: $ARIMA(0,1,1)x(2,1,1)_{12}$ 
* Model 8: $ARIMA(2,1,0)x(2,1,1)_{12}$ 

The models as mentioned earlier have the components of $(p,d,q)\times(P,D,Q)$ with the (p,P) as the Autoregressive portion, (d,D) as the differencing of trend and seasonality respectively, and (q,Q) as the Moving-average portion. The 12 which was indicated earlier as $m$ is the seasonal component that I had choosen and in this case would be one year. To assess each model, I fitted the least squares approach with a rank based on their AIC value. The model with the lowest AIC is the model that I would proceed forward with as the best fit and hopefully keep in mind to balance model complexity.

```{r, echo=FALSE}

#Fit candidate models
mod1 <- arima( emission, 
               order=c(1,1,1),
               seasonal=list(order=c(1,1,1),period=12))

mod2 <- arima( emission, 
               order=c(1,1,2),
               seasonal=list(order=c(1,1,1),period=12))

mod3 <- arima( emission, 
               order=c(0,1,1),
               seasonal=list(order=c(1,1,1),period=12))

mod4 <- arima( emission, 
               order=c(2,1,0),
               seasonal=list(order=c(1,1,1),period=12))

mod5 <- arima( emission, 
               order=c(1,1,1),
               seasonal=list(order=c(2,1,1),period=12))

mod6 <- arima( emission, 
               order=c(2,1,2),
               seasonal=list(order=c(2,1,1),period=12))

mod7 <- arima( emission, 
               order=c(0,1,1),
               seasonal=list(order=c(2,1,1),period=12))

mod8 <- arima( emission, 
               order=c(2,1,0),
               seasonal=list(order=c(2,1,1),period=12))


#Produce the results
data.frame( Model1 = mod1$aic,
            Model2 = mod2$aic,
            Model3 = mod3$aic,
            Model4 = mod4$aic,
            Model5 = mod5$aic,
            Model6 = mod6$aic,
            Model7 = mod7$aic,           
            Model8 = mod8$aic  )



```

Model 8, $ARIMA(2,1,0)x(2,1,1)_{12}$ showed to be the best fit with an AIC score of 1862.578. Next, I wanted to explore some diagnostics to assess if the model fit represented the data in a meaningful way.

####SARIMA Model Diagnostics

I started with the basic model diagnostic plots (Figure 12). The residuals did not appear to have any trend and the variance looked consistent throughout. The ACF showed no residual trend to the data and was sufficently 0 after one lag. looking into the p-values for the Ljung-Box statiscs the only area of concern might some of the p-values that hovered near 0.05. Since there were some p-values around the 0.05 area, I wanted to ensure that I had selected the best model and ran some diagnostics on several of the models and found their dianostics to be worse for various reasons.

```{r fig.height=6}

#Model 8 diagnostic plots
tsdiag(mod8, gof.lag=12)
title(sub = "Figure 12",
      cex.sub = 0.75, font.sub = 3, col.sub = "black")

```



Next I looked at the distribution of residuals; if the model is properly fitted, I knew that I should find that the residuals are relatively normal which is shown below Figure 13.

```{r, fig.height=3 }

#Plot the QQ plot
qqnorm(mod8$residuals, main = "Normal QQ Plot, Model 8 Residuals")
qqline(mod8$residuals)
title(sub = "Figure 13",
      cex.sub = 0.75, font.sub = 3, col.sub = "black")
```

Per the QQ plot above I had found the residuals to look normally distributed. 

With this being the main model of interest, I wanted to check this versus that of the auto.arima() function in R. This function would dynamically fit a model with numerious possibilities. I had choosen to use the criteria of AIC, to stay consistent with how my models were selected by hand. I also wanted to limit the complexity of the model to ensure that the computation time was reaasonable and I did this by maximizing the variables of $(p,P,q,Q)$ to 4.

```{r}
#Let R choose a model for us
mod.auto <- auto.arima(emission,
           d=1, D=1,  max.p = 4,
           max.q = 4, max.P = 4,
           max.Q = 4, ic="aic")

```

```{r}
mod.auto
```

As shown above,R had selected a model fit of $ARIMA(2,1,2)(4,1,2)_{12}$ as the best fit with the AIC is no better than that of model 8 earlier. The diagnostic plots (not shown) are similar. Because the model selected by R dynamically is more complex, and computationally longer I proceeded forward with the model that I had selected (model 8) since the results were visually the same.


###Spectral Analysis

In contrast to the above methods that focused on modelling in the time domain, spectral analysis focuses on the frequency domain. A brief exploratory analysis was performed below to assess the appropriateness of the ARIMA portion of the SARIMA model above (Model 8). 

Using the first order differenced series, I ploted, Figure 14, a periodogram to verify the seasonal component is annual.
```{r, echo=F, fig.height=3}
per.diff1 <- periodogram(diff1, main = "Periodogram of First-order Differenced Time Series")
title(sub = "Figure 14",
      cex.sub = 0.75, font.sub = 3, col.sub = "black")
```

This periodogram of the first-order differenced series shows peaks at frequencies that are multiples of 1/12, which confirmed the presence of an annual trend. These peaks are rather narrow, suggesting the seasonality is quite regular in this time series.

Next I used the combination of first and twelfth order differencing to plot the periodogram (Figure 15) of the stabilized residuals.

```{r, echo = F, fig.height=3}
per.diff12 <- periodogram(diff12, main = "Periodogram of Stabilized Residuals")
title(sub = "Figure 15",
      cex.sub = 0.75, font.sub = 3, col.sub = "black")
```

This periodogram shows an increasing spectral density up to a frequency of approximately 0.36, after which the density drops off while remaining greater than the lower frequency values. Smoothing of the Periodogram was not used so that I did not introduce any more complexity or bias to the fit.

Now that I had an idea of what the spectrum for the residuals of the time series looked like, it would be useful to compare the residual spectrum to the true spectrum from the non-seasonal part of the SARIMA model I fitted to the time series earlier. Recall that Model 8 took the form $SARIMA(2,1,0)\times(2,1,1)_{12}$. Since I had removed both trend and seasonality from this time series to generate a stable spectrum, I expected the AR(2) part of this model to reflect a similar spectral density as is shown in the periodogram of the residuals above. By calculating the spectrum of the AR(2) part of the model using the coefficients from Model 8, I got the following spectral density (Figure 16):

```{r, fig.height=3}
spec.AR2 <- ARMAspec(model = list(ar = c(mod8$coef[1], mod8$coef[2])), plot = T, col = "red", main = "True Spectral Density of AR(2) Component of model 8")
title(sub = "Figure 16",
      cex.sub = 0.75, font.sub = 3, col.sub = "black")

```

This appeared to match what the periodogram if Figure 15, but to be sure I chose to simulate the AR(2) component 1000 times and compare that average spectral density from the simulated periodograms to the true spectrum of the AR(2) model. This is shown below in Figure 17.
 

```{r, fig.height=3}

n <- 100
y <- arima.sim(n = n, model = list(ar = c(mod8$coef[1], mod8$coef[2])))
per.y <- periodogram(y, log = 'no', main = "AR(2), n = 500", plot = FALSE)

nsim <- 1000
spec.sim <- matrix(NA, nsim, length(per.y$freq))

for(isim in 1:nsim){
  y <- arima.sim(n = n, model = list(ar = c(mod8$coef[1], mod8$coef[2])))
  spec.sim[isim, ] <- periodogram(y, plot = F)$spec
}

spec.avg <- apply(spec.sim, 2, mean)
spec.sd <- apply(spec.sim, 2, sd)

plot(spec.AR2$freq, spec.AR2$spec, main = "Spectral Density of AR(2) Component of Model 8", xlab = "Frequency", ylab = "Spectral Density", type = "l", col = "red", lwd = 2, ylim = c(0, 10), xlim = c(0, 0.47))
lines(per.y$freq, spec.avg, lwd = 2, col = "blue")
legend(0,8, lty=c(1,1), col=c('red','blue'), legend=c("True","Simulated"))
title(sub = "Figure 17",
      cex.sub = 0.75, font.sub = 3, col.sub = "black")

```

The simulated spectral density of the AR(2) component takes a similar shape as the true spectral density, albeit somewhat overestimated. This similarity in the distribution at least helped me to support the notion that Model 8 was a good fit for this time series.


## Forecasting

As mentioned earlier that the purpose of this analysis was to create a prediciton of the next 12 months for the $CO_2$ emissions on gasoline powered motor vehicles. An effective use of time series modelling is to use a model to forecast future behavior of a time series. In this section, I generated forecasts with two methods: prediction from the SARIMA Model 8, and Exponential Smoothing via the Holt-Winters approach. Each method will attempt to forecast the next 24 months of $CO_2$ emissions.

###SARIMA Forecast
I began with the forecast of $CO_2$ emissions using SARIMA Model 8, along with confidence bands representing two standard errors.The plot of this prediction is shown below in Figure 18:

```{r, fig.height=3.5}

#Forecast 24 months
pred <- predict( mod8, n.ahead=24 )

plot(emission,
     xlim=c(2010,2016),
     ylim=c(70,110),
     xlab="Year",
     ylab="CO2, metric Megatons",
     main="2-Year CO2 Emission Forecast")
lines(pred$pred,
      col='red')
lines(pred$pred + 2 * pred$se, 
      col='blue',
      lty=1)
lines(pred$pred - 2 * pred$se, 
      col='blue',
      lty=1)
legend(2010,82, 
       lty=c(1,1),
       col=c('red','blue'),
       legend=c("Forecast","+/- 2 SE"))
title(sub = "Figure 18",
      cex.sub = 0.75, font.sub = 3, col.sub = "black")
```

After examining the visual output above in Figure 18 for the predictions from Model 8, I did not see anything that appeared to be concerning about the model fit. The prediction seemed to follow a conceivable output with the confidence inetervals expanding as time goes on which was to be expected.


###Exponential Smoothing: Holt-Winters

In contrast to the SARIMA method above, I could long-term prediction analysis with exponential smoothing. Since the $CO_2$ time series contains both trend and seasonality, I selected the Holt-Winter Seasonal Smoothing because it incorporates both aspects. Holt-Winters has the ability to optimize with the two updating equations and can be fit with either a multiplicative or additive model. Since the seasonal components of the time series showed stable variance over time, I decided to use the additive model.  

In R, I used the `forecast` library to calculate the smoothing estimates for the $\beta$ (trend) and $\gamma$ (seasonal) components of the Holt-Winters model. I specified a prediction level of 0.95 and a prediction period of 24 months. As shown in the plot below (Figure 19), the predicted values follow the most recent trend and seasonality, with a greater increase in upper and lower bounds over time. This predictive analysis looked almost identical visually to that produced by the SARIMA fit of Model 8.

```{r, echo=FALSE, fig.height=3}
fore_df_ts <- hw(df_ts, seasonal = "additive", h = 24, level = .95)
plot(fore_df_ts, xlim=c(2010,2016), ylim=c(70,110), main = "Holt-Winters Forecast, Additive Method", ylab = "CO2, metric Megatons", xlab = "Year")
title(sub = "Figure 19",
      cex.sub = 0.75, font.sub = 3, col.sub = "black")

#fore_df_ts$upper
#fore_df_ts$lower
#fore_df_ts$mean

```

---
#Again, we can view these forecast values in tabular format, which will be useful for comarison.
---

###Forecast Comparison

Visually, both forecasts seem to emulate the time series rather well. The Holt-Winters method projects the localized decreasing trend in the data that can be seen when looking at the most recent years from the time series, while it the seems to level off with no trend as time goes on.

To see if there was even a different I wanted to plot the two predictions together on one plot which is shown below in Figure 20:

```{r}

plot(fore_df_ts$mean, col = "blue", ylim = c(80, 100), xlim = c(2013.5, 2015.5), main = "Forecast Comparison", ylab = "CO2, metric Megatons", xlab = "Year")
lines(pred$pred, col = "red")
legend(2014.25,85, lty=c(1,1), col=c('red','blue'), legend=c("SARIMA","Holt-Winters"))
title(sub = "Figure 20",
      cex.sub = 0.75, font.sub = 3, col.sub = "black")

```

After I had looked at Figure 20, it does show that there is a slight differnce between the two model predictions. When looking closely I saw that the predictions actually separated more and more as time increased. This was not unsuprising given that the further a prediction is from the most recent known point in time the greater the uncertainty would be. Next, I wanted to look at the actual predictions and to see if there is a numerical difference between these two predictive models. So I created a dataframe that is shown below for all 24 predictive points. Column 1 is the prediction from the Holt-Winters model, Column 2 is the Prediction from the SARIMA model 8 model. To look at how these models compare, I looked at the upper and lower bands (I only used one standard error devation) for the SARIMA model 8 output which is in columns 3 and 4 respectively. Year of the predction was column 5 and the Holt-Winters in Range for column 6 is if the Holt-Winters prediction was within the one standard devation confidence range of the SARIMA model 8 preidciton. In all 24 preidction I found that the Holt-Winters was within that range which can be shown below, indicating that these two models are very similar in their prediction on this time series. 

```{r}

first_6 = cbind(replicate(6,"2013"))
second_12 =  cbind(replicate(12,"2014"))
last_6 = cbind(replicate(6,"2015"))


years = rbind(first_6, second_12, last_6)

data = as.data.frame(fore_df_ts$mean) %>% mutate(prediction = pred$pred, lower = pred$pred - pred$se, upper = pred$pred + pred$se , year = years)

names(data) = c("Holt-Winters", "Model 8 Prediction", "Model 8 Lower", "Model 8 Upper", "Years")

data %>% mutate("Holt-Winters in Range" = data$`Model 8 Lower`< data$`Holt-Winters` & data$`Holt-Winters` < data$`Model 8 Upper`)


```


From this analysis, these methods both seem to perform equally well in forecasting the 24-month period from July 2013 to June 2015 and no evidence to support that they are statistically different.   

#Conclusion. 

In this analysis, I explored multiple methods for modeling the $CO_2$ emissions from motor vehicles time series. I found that the decomposition/ARMA approach did not yield a stationary series and instead defered to a SARIMA model that appeared to better fit to the data. The spectrum anlaysis of the AR(2) component of this SARIMA model was then shown to agree with the spectrum of the $CO_2$ data within a reasonable margin, serving as validation that the SARIMA model was appropriate for the modelling of this time series. 

The SARIMA model was further tested by forecasting 24 months into the future and compared to the Holt-Winters exponential smoothing method. These methods agreed in their forecast of the future with high precision, both in terms of trend and seasonality. I therefore conclude that the monthly motor vehicle $CO_2$ emissions time series from January 1973 to June 2013 can successfully be modelled with a $SARIMA(2,1,0)\times(2,1,1)_{12}$ and that of the Holt-Winters. This predictive analysis could lead to further examinations of other models, more complex models or longer predictions. The next step would be to assess the actual predictions and see if the actual $CO_2$ commissions from July 2013 to June of 2015 actually landed in the ranges provided in the model.



#Appendix A - Source Code

```{r code=readLines(knitr::purl('./cabon_dioxide_redone.Rmd', documentation = 0)), eval = FALSE, echo=TRUE}

```












